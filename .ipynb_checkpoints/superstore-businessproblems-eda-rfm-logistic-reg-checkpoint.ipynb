{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Business Problems\n",
    "\n",
    "\n",
    "<b>Sales Performance Analysis</b>\n",
    "- How are the sales trending over time? Are there any noticeable seasonal patterns?\n",
    "- Which ship mode is most frequently chosen by customers?\n",
    "- What are the top-selling products or categories?\n",
    "\n",
    "<b>Geographical Insights</b>\n",
    "\n",
    "- Which countries, cities, or states contribute the most to the sales revenue?\n",
    "- Are there any specific regions where sales are consistently lower? What factors might be contributing to this?\n",
    "\n",
    "<b>Customer Segmentation</b>\n",
    "\n",
    "- Use the Recency, Frequency, and Monetary (RFM) framework to segment customers.\n",
    "- Segment customers based on their likelihood to churn (stop making purchases). Identify at-risk customers.\n",
    "\n",
    "<b>Product Analysis</b>\n",
    "- Are there specific products or sub-categories that are driving the majority of sales?\n",
    "- Are there product categories that are gaining or losing popularity among customers over time?\n",
    "\n",
    "<b>Order Processing Efficiency</b>\n",
    "- Are there specific shipping modes that tend to have quicker or slower processing times?\n",
    "\n",
    "\n",
    "\n",
    "# Loading necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:18:58.694960Z",
     "iopub.status.busy": "2023-09-08T10:18:58.693281Z",
     "iopub.status.idle": "2023-09-08T10:18:58.734975Z",
     "shell.execute_reply": "2023-09-08T10:18:58.733769Z",
     "shell.execute_reply.started": "2023-09-08T10:18:58.694899Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:18:58.737708Z",
     "iopub.status.busy": "2023-09-08T10:18:58.737242Z",
     "iopub.status.idle": "2023-09-08T10:18:58.968819Z",
     "shell.execute_reply": "2023-09-08T10:18:58.967895Z",
     "shell.execute_reply.started": "2023-09-08T10:18:58.737665Z"
    }
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xae in position 877: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSuperstore-Sales.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1235\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:75\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m     kwds\u001b[38;5;241m.\u001b[39mpop(key, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     74\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ensure_dtype_objs(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:544\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:633\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1952\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xae in position 877: invalid start byte"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Superstore-Sales.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:18:58.970965Z",
     "iopub.status.busy": "2023-09-08T10:18:58.970149Z",
     "iopub.status.idle": "2023-09-08T10:18:59.160375Z",
     "shell.execute_reply": "2023-09-08T10:18:59.158983Z",
     "shell.execute_reply.started": "2023-09-08T10:18:58.970930Z"
    }
   },
   "outputs": [],
   "source": [
    "num_samples = 3\n",
    "summary_data = []\n",
    "\n",
    "for column in df.columns:\n",
    "    null_count = df[column].isnull().sum()\n",
    "    unique_count = df[column].nunique()\n",
    "    data_type = df[column].dtype\n",
    "    min_value = df[column].min() if pd.api.types.is_numeric_dtype(data_type) else None\n",
    "    max_value = df[column].max() if pd.api.types.is_numeric_dtype(data_type) else None\n",
    "    mean = df[column].mean() if pd.api.types.is_numeric_dtype(data_type) else None\n",
    "    std_dev = df[column].std() if pd.api.types.is_numeric_dtype(data_type) else None\n",
    "    top_value = df[column].mode()[0] if pd.api.types.is_object_dtype(data_type) else None\n",
    "    top_value_freq = df[column].value_counts().max() if pd.api.types.is_object_dtype(data_type) else None\n",
    "    sample_values = df[column].sample(num_samples).tolist()\n",
    "    \n",
    "    summary_data.append([column, null_count, unique_count, data_type, min_value, max_value, mean, std_dev, top_value, top_value_freq, sample_values])\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data, columns=['Column', 'Null Count', 'Unique Count', 'Data Type', 'Min Value', 'Max Value', 'Mean', 'Std Dev', 'Top Value', 'Top Value Frequency', 'Sample Values'])\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data manipulation\n",
    "\n",
    "1. Change data type\n",
    "2. Fill nan values\n",
    "3. Drop unnecessary columns\n",
    "4. Sort values by Order Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:18:59.164169Z",
     "iopub.status.busy": "2023-09-08T10:18:59.163655Z",
     "iopub.status.idle": "2023-09-08T10:18:59.212767Z",
     "shell.execute_reply": "2023-09-08T10:18:59.211346Z",
     "shell.execute_reply.started": "2023-09-08T10:18:59.164130Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert data types to datetime\n",
    "df['Order Date'] = pd.to_datetime(df['Order Date'], format='%d/%m/%Y')\n",
    "df['Ship Date'] = pd.to_datetime(df['Ship Date'], format='%d/%m/%Y')\n",
    "\n",
    "# Drop columns\n",
    "df = df.drop(['Row ID'],axis=1)\n",
    "\n",
    "# Sort values by order date\n",
    "df.sort_values('Order Date', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:18:59.214824Z",
     "iopub.status.busy": "2023-09-08T10:18:59.214409Z",
     "iopub.status.idle": "2023-09-08T10:18:59.246848Z",
     "shell.execute_reply": "2023-09-08T10:18:59.245841Z",
     "shell.execute_reply.started": "2023-09-08T10:18:59.214790Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df['Postal Code'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vermont/Burlington is only one which have not postal code. I checked it and it is 05401."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:18:59.249159Z",
     "iopub.status.busy": "2023-09-08T10:18:59.248740Z",
     "iopub.status.idle": "2023-09-08T10:18:59.256397Z",
     "shell.execute_reply": "2023-09-08T10:18:59.255014Z",
     "shell.execute_reply.started": "2023-09-08T10:18:59.249119Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fillna values in 'Postal Code' with correct postal code\n",
    "df['Postal Code'] = df['Postal Code'].fillna(5401)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - How are the sales trending over time? Are there any noticeable seasonal patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:18:59.258795Z",
     "iopub.status.busy": "2023-09-08T10:18:59.258197Z",
     "iopub.status.idle": "2023-09-08T10:18:59.865940Z",
     "shell.execute_reply": "2023-09-08T10:18:59.864734Z",
     "shell.execute_reply.started": "2023-09-08T10:18:59.258600Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.dates import MonthLocator, DateFormatter\n",
    "\n",
    "order_date_df = df.set_index('Order Date') # We create a new df but this has 'Order Date' as index, that will help us\n",
    "\n",
    "# Quarterly sales\n",
    "quarterly_sales = order_date_df['Sales'].resample('Q').sum()\n",
    "quarterly_sales = quarterly_sales.round(2)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(quarterly_sales.index, quarterly_sales.values, marker='o', linestyle='-')\n",
    "\n",
    "# Customize x-axis ticks for quarterly intervals\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(MonthLocator(bymonth=[1, 4, 7, 10]))\n",
    "ax.xaxis.set_major_formatter(DateFormatter('%b %Y'))\n",
    "\n",
    "plt.title('Quarterly Sales Trend')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.xticks(rotation=60)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see, sales peak in January of almost every year. Almost every year, after January, sales decrease but start to increase again from April to January."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Which ship mode is most frequently chosen by customers? What is average sales per order for each ship mode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:18:59.868861Z",
     "iopub.status.busy": "2023-09-08T10:18:59.867644Z",
     "iopub.status.idle": "2023-09-08T10:19:00.159525Z",
     "shell.execute_reply": "2023-09-08T10:19:00.157851Z",
     "shell.execute_reply.started": "2023-09-08T10:18:59.868815Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new df of total sales of ship mode and count of ship modes columns\n",
    "ship_mode_df = df.groupby('Ship Mode').sum(numeric_only=True).sort_values('Sales', ascending=False)\n",
    "ship_mode_df = ship_mode_df[['Sales']]\n",
    "ship_mode_df['Count'] = df['Ship Mode'].value_counts()\n",
    "ship_mode_df.reset_index(inplace=True)\n",
    "\n",
    "plt.pie(ship_mode_df['Sales'], labels=ship_mode_df['Ship Mode'], autopct='%1.2f%%',\n",
    "        startangle=235, wedgeprops={'edgecolor':'black','linewidth':0.5})\n",
    "plt.title('Ship Modes Chosen by Customers')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see standard class has a percentile of 59.29, it is really high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:00.162975Z",
     "iopub.status.busy": "2023-09-08T10:19:00.161885Z",
     "iopub.status.idle": "2023-09-08T10:19:00.193972Z",
     "shell.execute_reply": "2023-09-08T10:19:00.192681Z",
     "shell.execute_reply.started": "2023-09-08T10:19:00.162902Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby('Ship Mode').mean(numeric_only=True).sort_values('Sales', ascending=False)[['Sales']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average sales per order for each ship mode are close to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - What are the top-selling products or categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:00.200022Z",
     "iopub.status.busy": "2023-09-08T10:19:00.199580Z",
     "iopub.status.idle": "2023-09-08T10:19:00.436826Z",
     "shell.execute_reply": "2023-09-08T10:19:00.435078Z",
     "shell.execute_reply.started": "2023-09-08T10:19:00.199987Z"
    }
   },
   "outputs": [],
   "source": [
    "category_df = df.groupby('Category').sum(numeric_only=True).sort_values('Sales', ascending=False)[['Sales']]\n",
    "category_df['Sales'] = category_df['Sales'].round(2)\n",
    "category_df.reset_index(inplace=True)\n",
    "\n",
    "colors = ['#94C9F2','#2F9CF0','#0A69B1']\n",
    "explode = [0.05, 0.05, 0.05]\n",
    "\n",
    "# display actual values instead of percentages\n",
    "def autopct_format(values):\n",
    "    def my_format(pct):\n",
    "        total = sum(values)\n",
    "        val = int(round(pct*total/100.0))\n",
    "        return '${v:d}'.format(v=val)\n",
    "    return my_format\n",
    "\n",
    "plt.pie(category_df['Sales'], labels=category_df['Category'],colors=colors,\n",
    "       explode=explode, autopct=autopct_format(category_df['Sales']),\n",
    "       pctdistance=0.80)\n",
    "\n",
    "circle = plt.Circle((0, 0), 0.65, facecolor='white')\n",
    "plt.gcf().gca().add_artist(circle)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:00.441336Z",
     "iopub.status.busy": "2023-09-08T10:19:00.439734Z",
     "iopub.status.idle": "2023-09-08T10:19:00.495067Z",
     "shell.execute_reply": "2023-09-08T10:19:00.493291Z",
     "shell.execute_reply.started": "2023-09-08T10:19:00.441254Z"
    }
   },
   "outputs": [],
   "source": [
    "product_df = df.groupby('Product Name').sum(numeric_only=True).sort_values('Sales', ascending=False)\n",
    "product_df = product_df[['Sales']]\n",
    "product_df['Sales'] = product_df['Sales'].round(2)\n",
    "product_df['Amount'] = df['Product Name'].value_counts()\n",
    "product_df.reset_index(inplace=True)\n",
    "product_df['Price'] = round(product_df['Sales']/product_df['Amount'],2)\n",
    "product_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top-selling category is Technology and top-selling product is 'Canon imageCLASS 2200 Advanced Copier'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geographical Insights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Which cities, or states contribute the most to the sales revenue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:00.497628Z",
     "iopub.status.busy": "2023-09-08T10:19:00.496772Z",
     "iopub.status.idle": "2023-09-08T10:19:01.244942Z",
     "shell.execute_reply": "2023-09-08T10:19:01.243694Z",
     "shell.execute_reply.started": "2023-09-08T10:19:00.497578Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group df by States and sum every sales in each state\n",
    "top_states = df.groupby('State')['Sales'].sum().reset_index()\n",
    "top_states['Sales'] = top_states['Sales'].round(1)\n",
    "top_states.sort_values(['Sales'], ascending=False, inplace=True)\n",
    "top_states = top_states.head(10)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.bar(top_states['State'], top_states['Sales'], color='#44A3EC',\n",
    "        edgecolor='k', linewidth=0.6)\n",
    "\n",
    "# Show values at top of each bar\n",
    "for i,value in  enumerate(top_states['Sales']):     \n",
    "    plt.text(i,value + 4000 ,'$'+ str(value), fontsize=11,\n",
    "                rotation=0,color='k', horizontalalignment='center')\n",
    "    \n",
    "plt.title('Top 10 States by Revenue', fontsize=20)    \n",
    "plt.xlabel('States', fontsize=15)\n",
    "plt.ylabel('Revenue ($)', fontsize=15)\n",
    "plt.xticks(fontsize=14, rotation=45)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:01.248365Z",
     "iopub.status.busy": "2023-09-08T10:19:01.246954Z",
     "iopub.status.idle": "2023-09-08T10:19:01.775380Z",
     "shell.execute_reply": "2023-09-08T10:19:01.773841Z",
     "shell.execute_reply.started": "2023-09-08T10:19:01.248299Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group df by Cities and sum every sales in each city\n",
    "top_cities = df.groupby(['City']).sum(numeric_only=True).sort_values('Sales', ascending=False)\n",
    "top_cities = top_cities[['Sales']].round(1)\n",
    "top_cities.reset_index(inplace=True)\n",
    "top_cities = top_cities.head(10)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.bar(top_cities['City'], top_cities['Sales'], color='#76F05B',\n",
    "        edgecolor='#3A722E', linewidth=1)\n",
    "\n",
    "# Show values at top of each bar\n",
    "for i,value in  enumerate(top_cities['Sales']):     \n",
    "  \n",
    "    plt.text(i,value/2-14000 ,'$'+ str(value), fontsize=11,\n",
    "                rotation=90,color='k', horizontalalignment='center')\n",
    "    \n",
    "plt.title('Top 10 Cities by Revenue', fontsize=20)    \n",
    "plt.xlabel('Cities', fontsize=15)\n",
    "plt.ylabel('Revenue ($)', fontsize=15)\n",
    "plt.xticks(fontsize=14, rotation=70)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see in top 10 states, California is the state that contribute the most to sales revenue. About the cities, New York City is the city that contribute the most to sales revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Are there any specific regions where sales are consistently lower? What factors might be contributing to this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:01.777556Z",
     "iopub.status.busy": "2023-09-08T10:19:01.777075Z",
     "iopub.status.idle": "2023-09-08T10:19:01.787166Z",
     "shell.execute_reply": "2023-09-08T10:19:01.785510Z",
     "shell.execute_reply.started": "2023-09-08T10:19:01.777525Z"
    }
   },
   "outputs": [],
   "source": [
    "df.Region.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:01.790757Z",
     "iopub.status.busy": "2023-09-08T10:19:01.788683Z",
     "iopub.status.idle": "2023-09-08T10:19:03.386263Z",
     "shell.execute_reply": "2023-09-08T10:19:03.385014Z",
     "shell.execute_reply.started": "2023-09-08T10:19:01.790698Z"
    }
   },
   "outputs": [],
   "source": [
    "# In the first section we created a order_date_df, we can use it to solve this problem\n",
    "west_df = order_date_df.loc[order_date_df['Region'] == 'West']\n",
    "east_df = order_date_df.loc[order_date_df['Region'] == 'East']\n",
    "south_df = order_date_df.loc[order_date_df['Region'] == 'South']\n",
    "central_df = order_date_df.loc[order_date_df['Region'] == 'Central']\n",
    "\n",
    "west_monthly_sales = west_df['Sales'].resample('M').sum()\n",
    "west_monthly_sales = west_monthly_sales.round(2)\n",
    "\n",
    "east_monthly_sales = east_df['Sales'].resample('M').sum()\n",
    "east_monthly_sales = east_monthly_sales.round(2)\n",
    "\n",
    "south_monthly_sales = south_df['Sales'].resample('M').sum()\n",
    "south_monthly_sales = south_monthly_sales.round(2)\n",
    "\n",
    "central_monthly_sales = central_df['Sales'].resample('M').sum()\n",
    "central_monthly_sales = central_monthly_sales.round(2)\n",
    "\n",
    "fig, (ax_west, ax_east, ax_south, ax_central) = plt.subplots(nrows=4, ncols=1, figsize=(12, 20))\n",
    "\n",
    "ax_west.plot(west_monthly_sales.index, west_monthly_sales.values, marker='o', linestyle='-')\n",
    "ax_east.plot(east_monthly_sales.index, east_monthly_sales.values, marker='o', linestyle='-')\n",
    "ax_south.plot(south_monthly_sales.index, south_monthly_sales.values, marker='o', linestyle='-')\n",
    "ax_central.plot(central_monthly_sales.index, central_monthly_sales.values, marker='o', linestyle='-')\n",
    "\n",
    "ax_west.set_title('West Region Monthly Sales')\n",
    "ax_east.set_title('East Region Monthly Sales')\n",
    "ax_south.set_title('South Region Monthly Sales')\n",
    "ax_central.set_title('Central Region Monthly Sales')\n",
    "ax_west.set_ylabel('Total Sales ($)')\n",
    "ax_central.set_xlabel('Months')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - <ins>West</ins>: There is an increase in overall sales, although occasionally there is a decrease in sales. In each January sales increased.\n",
    "#### - <ins>East</ins>: There is a continuous increase in 2018, but it has stagnated before. In each January sales increased.\n",
    "#### - <ins>South</ins>: Sales peaked in the spring of 2015, but remained stable until 2019.\n",
    "#### - <ins>Central</ins>: Sales peaked in the fall of 2015, then continued steadily until the fall of 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Use the Recency, Frequency, and Monetary (RFM) to segment customers.\n",
    "Recency: How recently did customers make a purchase?\n",
    "\n",
    "Frequency: How often do they make purchases?\n",
    "\n",
    "Monetary: What is the total monetary value of their purchases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:03.388975Z",
     "iopub.status.busy": "2023-09-08T10:19:03.387916Z",
     "iopub.status.idle": "2023-09-08T10:19:03.418179Z",
     "shell.execute_reply": "2023-09-08T10:19:03.417215Z",
     "shell.execute_reply.started": "2023-09-08T10:19:03.388926Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculating recency\n",
    "recency_df = df.groupby('Customer Name', as_index=False)['Order Date'].max()\n",
    "recent_date = recency_df['Order Date'].max()\n",
    "recency_df['Recency'] = recency_df['Order Date'].apply(\n",
    "lambda x: (recent_date - x).days)\n",
    "recency_df.rename(columns={'Order Date':'Last Purchase Date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:03.420217Z",
     "iopub.status.busy": "2023-09-08T10:19:03.419800Z",
     "iopub.status.idle": "2023-09-08T10:19:03.435382Z",
     "shell.execute_reply": "2023-09-08T10:19:03.433909Z",
     "shell.execute_reply.started": "2023-09-08T10:19:03.420176Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculating Frequency\n",
    "frequency_df = df.groupby('Customer Name', as_index=False)['Order Date'].count()\n",
    "frequency_df.rename(columns={'Order Date':'Frequency'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:03.438131Z",
     "iopub.status.busy": "2023-09-08T10:19:03.437169Z",
     "iopub.status.idle": "2023-09-08T10:19:03.455921Z",
     "shell.execute_reply": "2023-09-08T10:19:03.454617Z",
     "shell.execute_reply.started": "2023-09-08T10:19:03.438084Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculating monetary\n",
    "monetary_df = df.groupby('Customer Name', as_index=False)['Sales'].sum()\n",
    "monetary_df.rename(columns={'Sales':'Monetary'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:03.458594Z",
     "iopub.status.busy": "2023-09-08T10:19:03.457539Z",
     "iopub.status.idle": "2023-09-08T10:19:03.482277Z",
     "shell.execute_reply": "2023-09-08T10:19:03.480986Z",
     "shell.execute_reply.started": "2023-09-08T10:19:03.458544Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merging all three df in one df\n",
    "rfm_df = recency_df.merge(frequency_df, on='Customer Name')\n",
    "rfm_df = rfm_df.merge(monetary_df, on='Customer Name')\n",
    "rfm_df['Monetary'] = rfm_df['Monetary'].round(2)\n",
    "rfm_df.drop(['Last Purchase Date'], axis=1, inplace=True)\n",
    "\n",
    "rank_df = rfm_df.copy() # We make copy of rfm_df because we will need RFM features later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:03.484439Z",
     "iopub.status.busy": "2023-09-08T10:19:03.484052Z",
     "iopub.status.idle": "2023-09-08T10:19:03.511785Z",
     "shell.execute_reply": "2023-09-08T10:19:03.510350Z",
     "shell.execute_reply.started": "2023-09-08T10:19:03.484407Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalizing the rank of the customers\n",
    "rank_df['r_rank'] = rank_df['Recency'].rank(ascending=False)\n",
    "rank_df['f_rank'] = rank_df['Frequency'].rank(ascending=False)\n",
    "rank_df['m_rank'] = rank_df['Monetary'].rank(ascending=False)\n",
    "\n",
    "rank_df['r_rank_norm'] = (rank_df['r_rank'] / rank_df['r_rank'].max()) * 100\n",
    "rank_df['f_rank_norm'] = (rank_df['f_rank'] / rank_df['f_rank'].max()) * 100\n",
    "rank_df['m_rank_norm'] = (rank_df['m_rank'] / rank_df['m_rank'].max()) * 100\n",
    "\n",
    "rank_df.drop(['r_rank','f_rank','m_rank'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating RFM = 0.15 * Recency score + 0.28 * Frequency score + 0.57 * Monetary score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:03.515189Z",
     "iopub.status.busy": "2023-09-08T10:19:03.514085Z",
     "iopub.status.idle": "2023-09-08T10:19:03.527126Z",
     "shell.execute_reply": "2023-09-08T10:19:03.525482Z",
     "shell.execute_reply.started": "2023-09-08T10:19:03.515120Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculating RFM scores\n",
    "rank_df['rfm_score'] = (0.15*rank_df['r_rank_norm']) + (0.28*rank_df['f_rank_norm']) + (0.57*rank_df['m_rank_norm'])\n",
    "rank_df = rank_df[['Customer Name','rfm_score']]\n",
    "rank_df['rfm_score'] = round(rank_df['rfm_score']*0.05, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rating Customer based upon the RFM score\n",
    "- rfm score >4.5 : Top Customer\n",
    "- 4.5 > rfm score > 4 : High Value Customer\n",
    "- 4>rfm score >3 : Medium value customer\n",
    "- 3>rfm score>1.6 : Low-value customer\n",
    "- rfm score<1.6 :Lost Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:03.529562Z",
     "iopub.status.busy": "2023-09-08T10:19:03.529133Z",
     "iopub.status.idle": "2023-09-08T10:19:03.546831Z",
     "shell.execute_reply": "2023-09-08T10:19:03.545683Z",
     "shell.execute_reply.started": "2023-09-08T10:19:03.529531Z"
    }
   },
   "outputs": [],
   "source": [
    "# Masking all customers rfm scores by rating conditions to set customer segments easily\n",
    "top_customer_mask = (rank_df['rfm_score'] >= 4.5)\n",
    "high_value_mask = ((rank_df['rfm_score']<4.5) & (rank_df['rfm_score']>=4))\n",
    "medium_value_mask = ((rank_df['rfm_score']<4) & (rank_df['rfm_score']>=3))\n",
    "low_value_mask = ((rank_df['rfm_score']<3) & (rank_df['rfm_score']>=1.6))\n",
    "lost_mask = (rank_df['rfm_score'] < 1.6)\n",
    "\n",
    "rank_df.loc[top_customer_mask, 'Customer Segment'] = 'Top Customer'\n",
    "rank_df.loc[high_value_mask, 'Customer Segment'] = 'High Value Customer'\n",
    "rank_df.loc[medium_value_mask, 'Customer Segment'] = 'Medium Value Customer'\n",
    "rank_df.loc[low_value_mask, 'Customer Segment'] = 'Low Value Customer'\n",
    "rank_df.loc[lost_mask, 'Customer Segment'] = 'Lost Customer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:03.549013Z",
     "iopub.status.busy": "2023-09-08T10:19:03.548560Z",
     "iopub.status.idle": "2023-09-08T10:19:03.770895Z",
     "shell.execute_reply": "2023-09-08T10:19:03.767154Z",
     "shell.execute_reply.started": "2023-09-08T10:19:03.548979Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualization of customer segments\n",
    "colors = ['#0FABC4','#12C6E1','#5FDFEE','#85E8F7','#A6C5CA']\n",
    "plt.pie(rank_df['Customer Segment'].value_counts(), labels=rank_df['Customer Segment'].value_counts().index,\n",
    "       autopct='%.2f%%', pctdistance=0.8, labeldistance=1.1, colors=colors,\n",
    "       shadow=False, wedgeprops={'edgecolor':'k','linewidth':0.2})\n",
    "plt.title('Customer Segments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Segment customers based on their likelihood to churn (stop making purchases).\n",
    "\n",
    "- Features: ['Recency','Frequency','Monetary','Time since first purchase']\n",
    "- Target: ['Churn']\n",
    "\n",
    "We found recency, frequency and monetary in the previous section. \n",
    "\n",
    "We can find 'time since first purchase' with 'Order Date' column by: ['Order Date'].max() - customer first order.\n",
    "\n",
    "We can find churn by assuming lost customers=1 and other segments is 0. I mean 1 is churned customer and 0 is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:03.773380Z",
     "iopub.status.busy": "2023-09-08T10:19:03.772904Z",
     "iopub.status.idle": "2023-09-08T10:19:03.807319Z",
     "shell.execute_reply": "2023-09-08T10:19:03.805915Z",
     "shell.execute_reply.started": "2023-09-08T10:19:03.773333Z"
    }
   },
   "outputs": [],
   "source": [
    "#Find time since first purchase for every customer\n",
    "first_purchase_df = df.groupby('Customer Name', as_index=False)['Order Date'].min()\n",
    "first_purchase_df.rename(columns={'Order Date':'First Purchase Date'}, inplace=True)\n",
    "\n",
    "new_df = df.copy() # Make sure changes we will make doesn't affect original df so we copy it\n",
    "new_df = new_df.merge(first_purchase_df, on='Customer Name',how='left')\n",
    "new_df['Time Since First Purchase'] = (new_df['Order Date'].max() -\n",
    "                                        new_df['First Purchase Date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:03.809570Z",
     "iopub.status.busy": "2023-09-08T10:19:03.809128Z",
     "iopub.status.idle": "2023-09-08T10:19:03.846424Z",
     "shell.execute_reply": "2023-09-08T10:19:03.845368Z",
     "shell.execute_reply.started": "2023-09-08T10:19:03.809528Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add recancy,frequency,monetary and segment columns to df. We found those features in the previous section\n",
    "new_df = new_df.merge(rfm_df, on='Customer Name', how='left')\n",
    "new_df = new_df.merge(rank_df, on='Customer Name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:03.848641Z",
     "iopub.status.busy": "2023-09-08T10:19:03.848199Z",
     "iopub.status.idle": "2023-09-08T10:19:03.864689Z",
     "shell.execute_reply": "2023-09-08T10:19:03.863238Z",
     "shell.execute_reply.started": "2023-09-08T10:19:03.848605Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find churned and not churned customers\n",
    "churned = (new_df['Customer Segment'] == 'Lost Customer')\n",
    "not_churned = (new_df['Customer Segment'] != 'Lost Customer')\n",
    "\n",
    "new_df.loc[churned, 'Churned'] = 1\n",
    "new_df.loc[not_churned, 'Churned'] = 0\n",
    "new_df['Churned'] = new_df['Churned'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:03.867051Z",
     "iopub.status.busy": "2023-09-08T10:19:03.866397Z",
     "iopub.status.idle": "2023-09-08T10:19:03.882321Z",
     "shell.execute_reply": "2023-09-08T10:19:03.880592Z",
     "shell.execute_reply.started": "2023-09-08T10:19:03.867000Z"
    }
   },
   "outputs": [],
   "source": [
    "#Rename Churned column to Churn\n",
    "new_df.rename(columns={'Churned':'Churn'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:03.889689Z",
     "iopub.status.busy": "2023-09-08T10:19:03.889142Z",
     "iopub.status.idle": "2023-09-08T10:19:04.773035Z",
     "shell.execute_reply": "2023-09-08T10:19:04.771688Z",
     "shell.execute_reply.started": "2023-09-08T10:19:03.889645Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "features = ['Recency','Frequency','Monetary','Time Since First Purchase']\n",
    "target = 'Churn'\n",
    "\n",
    "# Create features and target \n",
    "X = new_df[features]\n",
    "y = new_df[target]\n",
    "\n",
    "# Split dataset in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression is one of the best model in churn prediction\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.3f}')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In summary model appears to have good overall performance with high accuracy, precision, and recall. However accuracy which is between 0.7-0.9 considered as ideal an realistic. But i am happy with the performance so i will leave it like this. We can use this model for further predictions on new dataset or at risk customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Are there specific products or sub-categories that are driving the majority of sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:04.775648Z",
     "iopub.status.busy": "2023-09-08T10:19:04.774808Z",
     "iopub.status.idle": "2023-09-08T10:19:05.059679Z",
     "shell.execute_reply": "2023-09-08T10:19:05.057969Z",
     "shell.execute_reply.started": "2023-09-08T10:19:04.775595Z"
    }
   },
   "outputs": [],
   "source": [
    "# We found product_df in Sales Performance Analysis so we can use that for this task and we already find top 10 total sales products\n",
    "product_top_10_amount = product_df.sort_values('Amount', ascending=False).head(10)\n",
    "\n",
    "def autopct_format(values):\n",
    "    def my_format(pct):\n",
    "        total = sum(values)\n",
    "        val = int(round(pct*total/100.0))\n",
    "        return '{v:d}'.format(v=val)\n",
    "    return my_format\n",
    "\n",
    "explodes = [0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05]\n",
    "plt.pie(product_top_10_amount['Amount'], labels=product_top_10_amount['Product Name'],\n",
    "       explode=explodes, autopct=autopct_format(product_top_10_amount['Amount']))\n",
    "circle = plt.Circle((0,0),0.8,color='w')\n",
    "plt.gcf().gca().add_artist(circle)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:05.063118Z",
     "iopub.status.busy": "2023-09-08T10:19:05.062119Z",
     "iopub.status.idle": "2023-09-08T10:19:05.662173Z",
     "shell.execute_reply": "2023-09-08T10:19:05.660494Z",
     "shell.execute_reply.started": "2023-09-08T10:19:05.063054Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.bar(df['Sub-Category'].value_counts().index, df['Sub-Category'].value_counts(),\n",
    "        color='#74e8e6',edgecolor='k', linewidth=0.6)\n",
    "\n",
    "plt.title('Top Selling Sub Category')\n",
    "plt.xlabel('Sub Categories')\n",
    "plt.ylabel('Amount')\n",
    "plt.xticks(rotation=70)\n",
    "\n",
    "for i,value in  enumerate(df['Sub-Category'].value_counts()):     \n",
    "  \n",
    "    plt.text(i,value+15 , str(value), fontsize=11,\n",
    "                rotation=0,color='k', horizontalalignment='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top selling product is staple envelop. Top selling sub category is binders. If we check both categories and products we can say that this store mainly sells office supplies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Are there categories that are gaining or losing popularity among customers over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:05.665211Z",
     "iopub.status.busy": "2023-09-08T10:19:05.664278Z",
     "iopub.status.idle": "2023-09-08T10:19:07.213452Z",
     "shell.execute_reply": "2023-09-08T10:19:07.211985Z",
     "shell.execute_reply.started": "2023-09-08T10:19:05.665169Z"
    }
   },
   "outputs": [],
   "source": [
    "# In dataset it is obvious that every row represent 1 quantity of product sale.\n",
    "#  So we create column named 'Quantity' and set it to 1\n",
    "order_date_df['Quantity'] = 1\n",
    "\n",
    "office_supplies_df = order_date_df.loc[order_date_df['Category'] == 'Office Supplies']\n",
    "technology_df = order_date_df.loc[order_date_df['Category'] == 'Technology']\n",
    "furniture_df = order_date_df.loc[order_date_df['Category'] == 'Furniture']\n",
    "\n",
    "# Find how many quantities sold per month for each category\n",
    "monthly_office = office_supplies_df['Quantity'].resample('M').sum()\n",
    "monthly_technology = technology_df['Quantity'].resample('M').sum()\n",
    "monthly_furniture = furniture_df['Quantity'].resample('M').sum()\n",
    "\n",
    "# Creating subplots for each category\n",
    "fig, (ax_off,ax_tech,ax_fur) = plt.subplots(nrows=3, ncols=1, figsize=(8,10))\n",
    "\n",
    "ax_off.plot(monthly_office.index, monthly_office.values, marker='o',linestyle='-')\n",
    "ax_tech.plot(monthly_technology.index, monthly_technology.values, marker='o',linestyle='-')\n",
    "ax_fur.plot(monthly_furniture.index, monthly_furniture.values, marker='o',linestyle='-')\n",
    "\n",
    "ax_off.set_title('Office Products Sales Quantities by Month')\n",
    "ax_tech.set_title('Technology Products Sales Quantities by Month')\n",
    "ax_fur.set_title('Furniture Products Sales Quantities by Month')\n",
    "ax_off.set_ylabel('Quantity')\n",
    "ax_fur.set_xlabel('Months')\n",
    "fig.tight_layout(pad=3.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In summary each category has very similar to others. Each category losing popularity after January but gaining popularity till next January in ecah year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order Processing Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Are there specific shipping modes that tend to have quicker or slower processing times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-08T10:19:07.215410Z",
     "iopub.status.busy": "2023-09-08T10:19:07.215043Z",
     "iopub.status.idle": "2023-09-08T10:19:07.271533Z",
     "shell.execute_reply": "2023-09-08T10:19:07.270249Z",
     "shell.execute_reply.started": "2023-09-08T10:19:07.215379Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Processing Time'] = (df['Ship Date'] - df['Order Date']).dt.days\n",
    "for unique in df['Ship Mode'].unique():\n",
    "    print(unique.upper())\n",
    "    print('Mean processing day' + ': ' + \n",
    "          str(round(df.loc[df['Ship Mode'] == unique]['Processing Time'].mean())))\n",
    "    print('Max processing day: ' + str(df.loc[df['Ship Mode'] == unique]['Processing Time'].max()))\n",
    "    print('Min processing day: ' + str(df.loc[df['Ship Mode'] == unique]['Processing Time'].min()))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In summary except same day, first class is the best in processing day as expected but if we check all the stats there is not big difference between ship modes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
